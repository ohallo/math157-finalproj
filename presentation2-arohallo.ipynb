{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Information Theory and Wordle 2/2 (Julia)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Overview\n",
    "So far we've defined information theory, the entropy function and the parameters of the game Wordle.\n",
    "\n",
    "Now the task is to find the best first guess in the game, that is the word with the highest entropy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Data\n",
    "There are two text files - one with the possible words that appear as answers in the game (which can be found by inspecting the wordle page) and one with a list of possible words that are accepted as guesses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12972-element Vector{String}:\n",
       " \"aahed\"\n",
       " \"aalii\"\n",
       " \"aargh\"\n",
       " \"aarti\"\n",
       " \"abaca\"\n",
       " \"abaci\"\n",
       " \"aback\"\n",
       " \"abacs\"\n",
       " \"abaft\"\n",
       " \"abaka\"\n",
       " \"abamp\"\n",
       " \"aband\"\n",
       " \"abase\"\n",
       " â‹®\n",
       " \"zowee\"\n",
       " \"zowie\"\n",
       " \"zulus\"\n",
       " \"zupan\"\n",
       " \"zupas\"\n",
       " \"zuppa\"\n",
       " \"zurfs\"\n",
       " \"zuzim\"\n",
       " \"zygal\"\n",
       " \"zygon\"\n",
       " \"zymes\"\n",
       " \"zymic\""
      ]
     },
     "execution_count": 1,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load list of possible answers (taken from the game's website)\n",
    "data = open(\"possible_words.txt\", \"r\")\n",
    "answers = readlines(data)\n",
    "\n",
    "# load list of possible guesses (valid five letter words that wordle will accept)\n",
    "data2 = open(\"allowed_guesses.txt\", \"r\")\n",
    "guesses = readlines(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2315, 12972)"
      ]
     },
     "execution_count": 2,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length(answers), length(guesses) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "There are 2,315 possible words that can be the Wordle solution. However, the game accepts 12,972 different five letter words (many of them being somewhat strange such as \"aahed\"). The task is to find which word among the list of words in 'guesses' will do the best job at reducing the outcome space since the smaller the outcome space the easier it is to guess the correct answer. Even by eyeballing the output of the list, it's clear that some words are better than others \"abase\" seems better than \"zuppa.\"\n",
    "\n",
    "If we can apply the entropy function to \"abase\" and \"zuppa\" and compare the number of bits each word gives, then we will have a numeric answer to this natural guess. In order to apply the entropy function to a given guess word, we need some sort of measurement of probability that the guess is the answer.\n",
    "\n",
    "One way to do this would be: for each of the 12,972 possible guesses, loop through all 2,315 possible answers compute the coloring vector as done in the previous notebook (represent each guess as a vector of 0's, 1's and 2's s.t. 0=grey, 1=yellow, 2=green) and compute the entropy of the distribution obtained by counting all the matches for a given guess then selecting which guess out of all guesses has the highest entropy. \n",
    "\n",
    "However, due to the length of the word sets, this approach is too time consuming so a more efficient method involves a 'pre-processing step' in which we loop over the solution set of words, and create a dictionary with the letters of the alphabet as keys, and the values of each key(letter) being a vector of 5 digits representing the number of times the letter occurs in the first, second, third, fourth and fifth position. What this allows us to do is get a rough estimate of what structure the best guess should have, meaning what letters are best in which position. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Making Letter Frequency Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{Any, Any} with 26 entries:\n",
       "  'n' => [37, 87, 139, 182, 130]\n",
       "  'f' => [136, 8, 25, 35, 26]\n",
       "  'w' => [83, 44, 26, 25, 17]\n",
       "  'd' => [111, 20, 75, 69, 118]\n",
       "  'e' => [72, 242, 177, 318, 424]\n",
       "  'o' => [41, 279, 244, 132, 58]\n",
       "  'h' => [69, 144, 9, 28, 139]\n",
       "  'j' => [20, 2, 3, 2, 0]\n",
       "  'i' => [34, 202, 266, 158, 11]\n",
       "  'k' => [20, 10, 12, 55, 113]\n",
       "  'r' => [105, 267, 163, 152, 212]\n",
       "  's' => [366, 16, 80, 171, 36]\n",
       "  't' => [149, 77, 111, 139, 253]\n",
       "  'q' => [23, 5, 1, 0, 0]\n",
       "  'y' => [6, 23, 29, 3, 364]\n",
       "  'a' => [141, 304, 307, 163, 64]\n",
       "  'c' => [198, 40, 56, 152, 31]\n",
       "  'p' => [142, 61, 58, 50, 56]\n",
       "  'm' => [107, 38, 61, 68, 42]\n",
       "  'z' => [3, 2, 11, 20, 4]\n",
       "  'g' => [115, 12, 67, 76, 41]\n",
       "  'v' => [43, 15, 49, 46, 0]\n",
       "  'l' => [88, 201, 112, 162, 156]\n",
       "  'u' => [33, 186, 165, 82, 1]\n",
       "  'x' => [0, 14, 12, 3, 8]\n",
       "  'b' => [173, 16, 57, 24, 11]"
      ]
     },
     "execution_count": 3,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loops over the answer list and counts the number of times each letter appears in each position\n",
    "letter_counts = Dict()\n",
    "for key in ['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z']\n",
    "    letter_counts[key] = [0,0,0,0,0]\n",
    "    for i in 1:5\n",
    "        for word in answers\n",
    "            if word[i]==key\n",
    "                letter_counts[key][i]+=1\n",
    "            end\n",
    "        end\n",
    "    end  \n",
    "end\n",
    "letter_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We can get a good idea for what kind of word appears most often as one of the wordle answers by selecting the letter that has the highest value in each of the five places. For example, the highest value in the first position is 366 which belongs to 's'. The highest value in the second position is '304' belonging to 'a'. The highest value in the third position is '307' belonging to 'a'. And '318' corresponding to 'e' for the fourth position and '424' corresponding to 'e' again in the fifth position. All together that is \"SAAEE\" which is not a word, but does tell us that S is a good starting letter and A in the second or third position and E in the fourth or fifth position is likely to be close to the answer. \n",
    "\n",
    "This dictionary also tells us what kinda of words are least likely to be the answer since none of the answers contained 'x' in the first position or 'j' or 'q' in the last position."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Computing Entropy of a Word Guess\n",
    "Let's test the hypothesis that 'abase' is a better word than 'zuppa' to use as a first guess. \n",
    "\n",
    "For a given word, we can compute its entropy by getting a distributions of the greens, yellows and greys \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "using StatsBase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "entropy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Computing the entropy of a given word is the hardest part of the task. We need to get a distribution of [greens, yellows, greys] for each individual letter then compute the entropy of the letter, then add up the  5 entropies of the five letters of the word. \n",
    "\n",
    "- First to get the 'greens' component of the distribution of the letter we just need to take the value in the dictionary corresponding to the index of that letter. \n",
    "- Next, the 'yellows' are obtained by taking the sum of the remaining values in the four other indices for that letter.\n",
    "- Lastly the 'greys' are just the number of words - yellows - greens\n",
    "\n",
    "Storing these three values for each letter gives a 3-element vector which is the distribution for that letter. The distribution becomes a probability by dividing each element by the total sum of the greens+yellows+greys. Finally, the entropy of that probability vector can be computed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "entropy_of_word (generic function with 1 method)"
      ]
     },
     "execution_count": 7,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function entropy_of_word(word, wordset)\n",
    "    H = 0\n",
    "    for (i,char) in enumerate(word)\n",
    "        greens = letter_counts[char][i]\n",
    "        yellows = sum(letter_counts[char]) - greens\n",
    "        greys = length(wordset) - yellows - greens\n",
    "        dist = [greens, yellows, greys]\n",
    "        dist = dist ./sum(dist)\n",
    "        H += entropy(dist)\n",
    "        end\n",
    "   return  H\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Comparing the entropy (# of bits gained) of \"abase\" vs \"zuppa\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3300237195818254"
      ]
     },
     "execution_count": 8,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy_of_word(\"abase\", guesses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7678557106567846"
      ]
     },
     "execution_count": 9,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy_of_word(\"zuppa\", guesses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "So as predicted,\"abase\" is a better guess than \"zuppa\" since it has more bits - meaning it does a better job at narrowing down the remaining words that the answer could be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Finding the Word with Maximum Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "find_best_word (generic function with 1 method)"
      ]
     },
     "execution_count": 10,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function find_best_word(wordset)\n",
    "    best_word = \"\"\n",
    "    best_entropy = 0\n",
    "    for word in wordset\n",
    "        H = 0\n",
    "        H += entropy_of_word(word, wordset)\n",
    "        if H > best_entropy\n",
    "            best_entropy = H\n",
    "            best_word = word\n",
    "        end\n",
    "    end\n",
    "    return(best_word, best_entropy)\n",
    "    end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Let's see how the function compares when we run it on the answer set vs the allowed guess set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"areae\", 1.622623743402735)"
      ]
     },
     "execution_count": 11,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#searching for highest entropy among the 12,972 possible guesses\n",
    "find_best_word(guesses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"tease\", 4.480724406556044)"
      ]
     },
     "execution_count": 12,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#searching for highest entropy among the 2,314 possible answers\n",
    "find_best_word(answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "From the results \"tease\" is clearly the better word to use as a first guess since it's entropy is over three times as high and it also contains more distinct letters. However the best guess still doesn't seem entirely correct since ideally the best first guess would contain 5 distinct letters and tease only contains four distinct letters. \n",
    "\n",
    "There are a couple explanations for these results.\n",
    "1. Going back to the 'simplified' model created - it does not take into account double letters exactly like Wordle does. Mainly, if a letter in a guess is repeated n times but occurs m times in the actual answer with m<n, then Wordle will only color the guess with green/yellow up to m times. I did not create edge cases in the entropy function to handle certain situations which occur when there are repeated letters - but I linked two references that handled those situations.\n",
    "2. The results are almost always better when a model is 'trained' on the 'test' set. Meaning, since I fed the function `find_best_word` the answer list, it's not surprising that it returned a better word than when the possible guess list was used. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Solution Recap \n",
    "\n",
    "Information theory and the entropy function can be used to find the word with maximum entropy by the following steps:\n",
    "\n",
    "1. Iterate over the possible answers and count the number of times each letter of the alphabet shows up in the first, second, third, fourth and fifth position\n",
    "2. With these counts, we can get an estimate for the probability of getting a green, yellow and grey match for the letters a given guess and store it in a three element vector to represent the distribution\n",
    "3. Compute the entropy of the distribution of green, yellow and grey for each letter and sum the entropies for the five letters of the word\n",
    "4. The word among the guesses with the highest entropy is the best word guess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Additional Resources\n",
    "Information theory has other applications to different areas of cryptography, computer science and physics. \n",
    "\n",
    "There are a lot of different implementations online of Wordle solvers using information theory - interestingly most find different 'best' words\n",
    " - https://aditya-sengupta.github.io/coding/2022/01/13/wordle.html (explains the edge case with repeated letters)\n",
    " - https://www.youtube.com/watch?v=fRed0Xmc2Wg&t=1s (explains the edge case with repeated letters)\n",
    " - https://betterprogramming.pub/forget-luck-optimized-wordle-strategy-using-bigquery-c676771e316f (uses Google BigQuery)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7",
   "env": {
    "JULIA_DEPOT_PATH": "/home/user/.julia/:/ext/julia/depot/",
    "JULIA_PROJECT": "/home/user/.julia/environment/v1.7"
   },
   "language": "julia",
   "metadata": {
    "cocalc": {
     "description": "The Julia Programming Language",
     "priority": 10,
     "url": "https://julialang.org/"
    }
   },
   "name": "julia-1.7",
   "resource_dir": "/ext/jupyter/kernels/julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}